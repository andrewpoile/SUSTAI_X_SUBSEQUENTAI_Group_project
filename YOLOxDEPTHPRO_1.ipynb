{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import ultralytics\n",
    "ultralytics.checks()\n",
    "from ultralytics import YOLO\n",
    "model = YOLO(\"yolo11x.pt\")\n",
    "\n",
    "import torch\n",
    "import torch.version\n",
    "import torchvision\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"Torchvision version:\", torchvision.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "\n",
    "#from PIL import Image\n",
    "import depth_pro\n",
    "\n",
    "# Load depth model and preprocessing transform\n",
    "depth_model, transform = depth_pro.create_model_and_transforms(device=\"cuda\",precision=torch.half)\n",
    "depth_model.eval()\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Group:\n",
    "    def __init__(self,group,frameid):\n",
    "        self.groupcount = group.shape\n",
    "        self.frameid = frameid\n",
    "        pass\n",
    "\n",
    "    def update(self,):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_l2_norms(centroids, threshold=10.0, device='cuda'):\n",
    "    \"\"\"\n",
    "    Vectorized function to calculate pairwise L2 norms between centroids (3D) on GPU\n",
    "    and return group memberships based on a distance threshold.\n",
    "\n",
    "    Parameters:\n",
    "    centroids (torch.Tensor): A tensor of shape (N, 3) representing object centroids (x, y, z) for the current frame.\n",
    "    threshold (float): The distance threshold to consider objects in the same group.\n",
    "    device (str): Device where the tensors are stored ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "    group_memberships (list of sets): List of sets, each set contains indices of objects that are in the same group.\n",
    "    \"\"\"\n",
    "    centroids = centroids.to(device)  # Move centroids tensor to the appropriate device\n",
    "\n",
    "    # Compute pairwise L2 distances using broadcasting in PyTorch\n",
    "    diff = centroids[:, None, :] - centroids[None, :, :]  # Shape (N, N, 3)\n",
    "    dist_matrix = torch.norm(diff, dim=2)  # Shape (N, N), compute the L2 norm along axis 2\n",
    "\n",
    "    # Create a list of groups based on the distance threshold\n",
    "    num_objects = centroids.shape[0]\n",
    "    group_memberships = []\n",
    "\n",
    "    for i in range(num_objects):\n",
    "        group = set([i])  # Start with the current object in its own group\n",
    "        for j in range(num_objects):\n",
    "            if i != j and dist_matrix[i, j] < threshold:\n",
    "                group.add(j)\n",
    "        group_memberships.append(group)\n",
    "\n",
    "    return group_memberships\n",
    "\n",
    "def track_groups(previous_groups, current_groups):\n",
    "    \"\"\"\n",
    "    Function to track if objects have joined or left groups between frames.\n",
    "\n",
    "    Parameters:\n",
    "    previous_groups (list of sets): List of sets from the previous frame's group memberships.\n",
    "    current_groups (list of sets): List of sets from the current frame's group memberships.\n",
    "\n",
    "    Returns:\n",
    "    joined (list): List of object indices that joined a new group.\n",
    "    left (list): List of object indices that left a group.\n",
    "    \"\"\"\n",
    "    joined = []\n",
    "    left = []\n",
    "\n",
    "    # Create sets to track which objects have changed groups\n",
    "    prev_indices = {frozenset(group) for group in previous_groups}\n",
    "    curr_indices = {frozenset(group) for group in current_groups}\n",
    "\n",
    "    # Detect objects joining new groups\n",
    "    for i, current_group in enumerate(current_groups):\n",
    "        if frozenset(current_group) not in prev_indices:\n",
    "            joined.append(i)\n",
    "\n",
    "    # Detect objects leaving groups\n",
    "    for i, prev_group in enumerate(previous_groups):\n",
    "        if frozenset(prev_group) not in curr_indices:\n",
    "            left.append(i)\n",
    "\n",
    "    return joined, left\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop-based grouping with depth-based inter-object distance estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.track(source=\"Homebrew-video/Low-quality/IMG_5347.MP4\",stream=True,classes=[0],half=True,imgsz=1280,vid_stride=1)\n",
    "frame=1\n",
    "for j,r in enumerate(results):\n",
    "    #if j==1: break\n",
    "    #r.to(\"cuda\")\n",
    "\n",
    "    # Load and preprocess given frame.\n",
    "    d_img = transform(r.orig_img).to(\"cuda\") # Transform and transfer given frame to gpu to be ready for depth inference.\n",
    "    f_px = depth_pro.utils.fpx_from_f35(d_img.shape[2],d_img.shape[1],26) # Converts focal length from my phone camera to fpx.\n",
    "\n",
    "    # Run depth inference on given frame.\n",
    "    prediction = depth_model.infer(d_img,f_px)\n",
    "    depth = prediction[\"depth\"]  # Depth in [m].\n",
    "    focallength_px = prediction[\"focallength_px\"]  # Focal length in pixels.\n",
    "\n",
    "    # Initialise tensor to store bounding box centroids and depth estimates.\n",
    "    richard=harry = torch.empty(0,3).to(\"cuda\")\n",
    "    start = time.time()\n",
    "    # Loop over bounding boxes detected in given frame.\n",
    "    for i in r.boxes:\n",
    "        xy = i.xyxy[0]#; print(xy); print(xy[:2],xy[2:])\n",
    "        c = torch.tensor([ (xy[:2][0]+xy[2:][0])/2, (xy[2:][-1]+xy[:2][-1])/2 ], device=\"cuda\"); print(\"Centroid coordinates (px)\",c)\n",
    "        c = i.xywh[0,:2].to(\"cuda\") ;print(c)\n",
    "        d = depth[int(c[1].item())][int(c[0].item())]; print(\"Depth estimate at centroid (m)\",d)\n",
    "        f = focallength_px\n",
    "        id = i.id ;print(i.id)\n",
    "        cr = c * d/f # Centroids coordinates in estimated metric distance.\n",
    "        \n",
    "        blobs = torch.hstack((c,d)) ;print(blobs)\n",
    "        blobs2= torch.hstack((cr,d)) ;print(blobs2)\n",
    "        richard = torch.vstack((richard,blobs)) ;print(\"this is richard say hello:\", richard)\n",
    "        harry = torch.vstack((harry,blobs2)) ;print(\"this is harry say hello:\", harry)\n",
    "    #dist = calculate_l2_norms(richard); print(\"these are the groups:\", dist)\n",
    "\n",
    "    clustering = DBSCAN(eps=4, min_samples=2).fit(harry.cpu().numpy())\n",
    "    labs = clustering.labels_\n",
    "    print(labs)\n",
    "    end = time.time()\n",
    "    print(\"Time to loop over all boxes\",end-start)\n",
    "    \n",
    "    plt.imshow(r.orig_img)\n",
    "    scatter = plt.scatter(richard.cpu().numpy()[:,0],richard.cpu().numpy()[:,1],c=labs,label=labs)\n",
    "    plt.legend(*scatter.legend_elements(),\n",
    "               title=\"Classes\")\n",
    "    #plt.xlim(0,1280)\n",
    "    #plt.ylim(0,720)\n",
    "    plt.tight_layout\n",
    "    plt.savefig(f\"runs/dots3/frame_{frame}\",dpi=200)\n",
    "    frame += 1\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelised grouping with depth-based inter-object distance estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.track(source=\"Homebrew-video/Low-quality/IMG_5354.MP4\",\n",
    "                      stream=True,classes=[0],half=True,imgsz=1280,vid_stride=1,\n",
    "                      conf=0.25,iou=0.9)\n",
    "frame=1\n",
    "for j,r in enumerate(results):\n",
    "    #if j==60: break\n",
    "    r = r.to(\"cuda\")\n",
    "\n",
    "    # Load and preprocess given frame.\n",
    "    d_img = transform(r.orig_img) # Transform and transfer given frame to gpu to be ready for depth inference.\n",
    "    f_px = depth_pro.utils.fpx_from_f35(d_img.shape[2],d_img.shape[1],26) # Converts focal length from my phone camera to fpx.\n",
    "\n",
    "    # Run depth inference on given frame.\n",
    "    prediction = depth_model.infer(d_img,f_px)\n",
    "    depth = prediction[\"depth\"]  # Depth in [m].\n",
    "    focallength_px = prediction[\"focallength_px\"]  # Focal length in pixels.\n",
    "\n",
    "    # Initialise tensor to store bounding box centroids and depth estimates.\n",
    "    groups = torch.empty(0,4).to(\"cuda\")\n",
    "\n",
    "    # Parallelised centroid extraction and depth assignment\n",
    "    c = r.boxes.xywh[:,:2] ;print(\"Centroid coordinates (px):\", c)\n",
    "    d = depth[c[:,1].int(),c[:,0].int()] ;print(\"Depth at each centroid:\", d)\n",
    "    f = focallength_px ;print(\"Focal length (px):\", f)\n",
    "    cr = c.T * d/f ;print(\"Metric centroid coordinates (m):\",cr)\n",
    "    groups = torch.vstack((cr,d)) ;print(\"Bounding box x,y,z coordinates (m):\",groups)\n",
    "\n",
    "    # L2 Norm\n",
    "    # norms = calculate_l2_norms(groups.T,2) ;print(\"L2 norm code grouping:\",norms)\n",
    "\n",
    "    # Clustering 3D points\n",
    "    clustering = DBSCAN(eps=2, min_samples=2).fit(groups.T.cpu().numpy())\n",
    "    labs = clustering.labels_ ;print(\"Clustering labels:\",labs)\n",
    "\n",
    "    # Prepare groups for tracking group momentum\n",
    "    groups = torch.vstack((groups,r.boxes.id,torch.tensor(labs,device=\"cuda\"))) ;print(\"Groups with id appended:\",groups.T)\n",
    "    \n",
    "    # Count frames and modulate group membership\n",
    "    group_class = Group(groups,frame)\n",
    "    \n",
    "\n",
    "    # Saving the images of centroids and group ownership\n",
    "    plt.imshow(r.orig_img)\n",
    "    scatter = plt.scatter(c.cpu().numpy()[:,0],c.cpu().numpy()[:,1],c=labs,label=labs)\n",
    "    plt.legend(*scatter.legend_elements(),title=\"Classes\")\n",
    "    plt.tight_layout\n",
    "    plt.savefig(f\"runs/dots4/frame_{frame}\",dpi=100)\n",
    "    plt.clf()\n",
    "    frame += 1\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SUSTxSUBSEQUENT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
