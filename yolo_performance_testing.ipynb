{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision.io import read_video\n",
    "from torchvision.transforms.functional import resize,normalize\n",
    "import ultralytics\n",
    "import depth_pro\n",
    "\n",
    "ultralytics.checks()\n",
    "model = ultralytics.YOLO(\"yolo11x-pose.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "rat = a/r\n",
    "print(r,a,f,rat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load video to tensor\n",
    "v_frames = read_video(\"Homebrew-video/Low-quality/IMG_5347.MP4\",output_format=\"TCHW\")[0].to(\"cuda\").half()/255\n",
    "v_frames = normalize(v_frames,[0.5,0.5,0.5],[0.5,0.5,0.5],inplace=True)\n",
    "#v_frames = DataLoader(v_frames)\n",
    "#v_frames = DataLoader(TensorDataset(v_frames),batch_size=v_frames.shape[0])\n",
    "#v_frames = read_video(\"Homebrew-video/Low-quality/IMG_5347.MP4\")[0].to(\"cpu\")\n",
    "#print(v_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load depth model and preprocessing transform\n",
    "depth_model, transform = depth_pro.create_model_and_transforms(device=\"cuda\",precision=torch.half)\n",
    "depth_model.eval()\n",
    "print(\"all good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image,\n",
    "\n",
    "for i,v_frame in enumerate(v_frames):\n",
    "    depth_in = time.time()\n",
    "\n",
    "    # Run inference.\n",
    "    prediction = depth_model.infer(v_frame)\n",
    "    depth = prediction[\"depth\"]  # Depth in [m].\n",
    "    focallength_px = prediction[\"focallength_px\"]  # Focal length in pixels.\n",
    "    \n",
    "    depth_out = time.time()\n",
    "    \n",
    "    print(depth_out-depth_in)\n",
    "    \n",
    "    #print(depth)\n",
    "    #save_image(v_frame,f\"frame_{i}.png\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_in = time.time()\n",
    "# Run inference.\n",
    "prediction = depth_model.infer(v_frames)\n",
    "depth = prediction[\"depth\"]  # Depth in [m].\n",
    "focallength_px = prediction[\"focallength_px\"]  # Focal length in pixels.\n",
    "\n",
    "depth_out = time.time()\n",
    "\n",
    "print(depth_out-depth_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run online object detection\n",
    "results = model.track(source=\"Homebrew-video/Low-quality/IMG_5347.MP4\",classes=[0],stream=True)\n",
    "start = time.time()\n",
    "# Loop over YOLO results and call inference on depth pro\n",
    "for i,r in enumerate(results):\n",
    "    depth_in = time.time()\n",
    "    # Transform image\n",
    "    #d_img = transform(v_frames[i])\n",
    "    # Run inference.\n",
    "    prediction = depth_model.infer(v_frames[i])\n",
    "    depth = prediction[\"depth\"]  # Depth in [m].\n",
    "    focallength_px = prediction[\"focallength_px\"]  # Focal length in pixels.\n",
    "    depth_out = time.time()\n",
    "    print(depth_out-depth_in)\n",
    "    end = time.time()\n",
    "    print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#beeg = resize(v_frames,(640,640)) ; print(beeg.shape)\n",
    "#results = model.predict(source=beeg, save=True)\n",
    "\n",
    "for frame in v_frames:\n",
    "    frame = resize(frame,600,max_size=640).unsqueeze(0)/255\n",
    "    results = model.track(source=frame,save=True,classes=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SUSTxSUBSEQUENT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
