{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import ultralytics\n",
    "ultralytics.checks()\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.trackers.bot_sort import BOTSORT\n",
    "from ultralytics.utils.plotting import Annotator\n",
    "from ultralytics.engine.results import Boxes\n",
    "\n",
    "import torch\n",
    "from torch.nn.functional import normalize\n",
    "import torch.version\n",
    "import torchvision\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"Torchvision version:\", torchvision.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "\n",
    "#from PIL import Image\n",
    "import depth_pro\n",
    "\n",
    "# Load depth model and preprocessing transform\n",
    "depth_model, transform = depth_pro.create_model_and_transforms(device=\"cuda\",precision=torch.half)\n",
    "depth_model.eval()\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from no_hungarian2 import ObjectTracker\n",
    "\n",
    "class Detection:\n",
    "    def __init__(self,detection):\n",
    "        pass\n",
    "\n",
    "class Group:\n",
    "    def __init__(self,\n",
    "                 occlusion_distance:float=0.5,\n",
    "                 remove_distance:float=5.0,\n",
    "                 velocity_threshold:float=2.0,\n",
    "                 max_occlusion_frames:int=10\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        occlusion_distance: max distance to consider an occlusion\n",
    "        remove_distance: max distance to keep an object before removing\n",
    "        velocity_threshold: max diff from group vel to join as new member\n",
    "        max_occlusion_frames: frames to keep an occluded object before removal\n",
    "        \"\"\"\n",
    "        self.occlusion_distance = occlusion_distance\n",
    "        self.remove_distance = remove_distance\n",
    "        self.velocity_threshold = velocity_threshold\n",
    "        self.max_occlusion_frames = max_occlusion_frames\n",
    "\n",
    "        self.next_id = 0\n",
    "        # Objects: id -> info dict\n",
    "        self.objects = {}\n",
    "        # Groups: cluster_label -> set of ids\n",
    "        self.groups = defaultdict(set)\n",
    "    \n",
    "    def _assign_id(self):\n",
    "        _id = self.next_id\n",
    "        self.next_id += 1\n",
    "        return _id\n",
    "\n",
    "    def update(self,detections):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depth_pro_inference(r,f35:int=26):\n",
    "    \"\"\"\n",
    "    Depth pro inference step.\n",
    "    \"\"\"\n",
    "    # Load and preprocess given frame.\n",
    "    d_img = transform(r.orig_img) # Transform and transfer given frame to gpu to be ready for depth inference.\n",
    "    f_px = depth_pro.utils.fpx_from_f35(d_img.shape[2],d_img.shape[1],f35) # Converts focal length from camera to fpx.\n",
    "\n",
    "    # Run depth inference on given frame.\n",
    "    prediction = depth_model.infer(d_img,f_px)\n",
    "    depth = prediction[\"depth\"]  # Depth in [m].\n",
    "    focallength_px = prediction[\"focallength_px\"]  # Focal length in pixels.\n",
    "\n",
    "    return depth,focallength_px\n",
    "\n",
    "def BoTSORT_tracker_update(r,tracker):\n",
    "    \"\"\"\n",
    "    BoTSORT tracker update.\n",
    "    \"\"\"\n",
    "    # Velocity extraction from BoTSORT Kalman filter\n",
    "    _ = tracker.update(r.boxes.cpu(),r.orig_img)\n",
    "    # Extract Kalman states\n",
    "    kalman_states = {\n",
    "        track.track_id: track.mean.copy()\n",
    "        for track in tracker.tracked_stracks\n",
    "        if track.is_activated\n",
    "    }\n",
    "\n",
    "    return kalman_states\n",
    "\n",
    "def speed_estimation(r,speedestimator):\n",
    "    \"\"\"\n",
    "    Speed estimation using yolo solutions.\n",
    "    \"\"\"\n",
    "    # Speed estimation\n",
    "    speedest = speedestimator(r.orig_img)\n",
    "    speeds = speedest.speed_dict ;print(\"Speed:\",speeds)\n",
    "    angles = speedest.angle_dict ;print(\"Angle:\",angles)\n",
    "\n",
    "    return speeds,angles\n",
    "\n",
    "def theloop(\n",
    "        r,\n",
    "        d,\n",
    "        f,\n",
    "        tracker,\n",
    "        grouptracker,\n",
    "        save_img:bool=False,\n",
    "        run:int=6\n",
    "        ):\n",
    "    \"\"\"\n",
    "    The loop.\n",
    "    \"\"\"\n",
    "    # Extract Kalman means\n",
    "    kalman_states = BoTSORT_tracker_update(r,tracker) ;print(\"Detection Kalman states:\",kalman_states)\n",
    "    x = torch.tensor([list(x)[0:2] for x in kalman_states.values()])[:,0] ;print(\"I am x:\",x)\n",
    "    y = torch.tensor([list(x)[0:2] for x in kalman_states.values()])[:,1] ;print(\"I am y:\",y)\n",
    "    u = torch.tensor([list(x)[4:6] for x in kalman_states.values()])[:,0] ;print(\"I am u:\",u)\n",
    "    v = torch.tensor([list(x)[4:6] for x in kalman_states.values()])[:,1] ;print(\"I am v:\",v)\n",
    "    \n",
    "    # Parallelised centroid extraction and depth assignment\n",
    "    d = d[y.int(),x.int()].cpu() ;print(\"\\nDepth at each centroid:\", d)\n",
    "    f = f ;print(\"Focal length (px):\", f)\n",
    "    xr = x * d/f ;print(\"Metric x coordinates (m):\",xr)\n",
    "    yr = y * d/f ;print(\"Metric y coordinates (m):\",yr)\n",
    "    lumps = torch.vstack((xr,yr,d)) ;print(\"Bounding box x,y,z coordinates (m):\",lumps)\n",
    "    \n",
    "    # Clustering 3D points\n",
    "    xyz_clustering = DBSCAN(eps=2, min_samples=2).fit(lumps.T.cpu().numpy())\n",
    "    xyz_labs = xyz_clustering.labels_ ;print(\"\\nClustering labels:\",xyz_labs)\n",
    "\n",
    "    # For each spatial cluster, further cluster by velocity.\n",
    "    vlumps = torch.vstack((u,v,torch.tensor(xyz_labs))) ;print(\"Vlumps:\",vlumps)\n",
    "    cids  = vlumps.T[:,-1]\n",
    "    vids = torch.full_like(cids,-1).long()\n",
    "    \n",
    "    for cid in cids.unique():\n",
    "        # Ignore noise outputs from previous clustering\n",
    "        if cid == -1:\n",
    "            continue\n",
    "        # Create mask for processing and reconstruction of velocity cluster labels\n",
    "        mask = cids == cid\n",
    "        vels = normalize(vlumps.T[mask,:-1]).tolist() ;print(\"Normalised velocities:\",vels)\n",
    "        # Velocity Clustering\n",
    "        vclustering = DBSCAN(eps=0.3, min_samples=2).fit(vels) ;print(\"Velocity clustering labels:\",vclustering.labels_)\n",
    "        vlabs = [( -1 if i==-1 else int(i+(cid*100)) ) for i in vclustering.labels_] # Distinguish spatial groups\n",
    "        vids[mask] = torch.tensor(vlabs)\n",
    "    uvxyz_labs = vids.tolist() ;print(\"uvxyz_labs:\",uvxyz_labs)\n",
    "\n",
    "    # Initiate annotator\n",
    "    annotator = Annotator(r.orig_img)\n",
    "\n",
    "    # Extracting the pixel values from the kalman state\n",
    "    kal_vals = torch.tensor(list(kalman_states.values()))\n",
    "    group_boxes = []\n",
    "\n",
    "    # Produce bounding boxes for each group\n",
    "    for vid in vids.unique():\n",
    "        # Ignore noise outputs from previous clustering\n",
    "        if vid == -1:\n",
    "            continue\n",
    "        # Create mask for processing and reconstruction of velocity cluster labels\n",
    "        mask = vids == vid\n",
    "        values = kal_vals[mask,:4]\n",
    "        X1,Y1,X2,Y2 = (min(values[:,0]-values[:,2]/2),min(values[:,1]-values[:,3]/2),\n",
    "                       max(values[:,0]+values[:,2]/2),max(values[:,1]+values[:,3]/2))\n",
    "        # Annotate frame with group boxes\n",
    "        annotator.box_label((X1,Y1,X2,Y2),f\"Group: {vid}, Population: {values.dim()}\",(0,180,255))\n",
    "        # Make boxes for passing to ultralytics Boxes call\n",
    "        raw_box = [min(values[:,0]-values[:,2]/2),\n",
    "                   min(values[:,1]-values[:,3]/2),\n",
    "                   max(values[:,0]+values[:,2]/2),\n",
    "                   max(values[:,1]+values[:,3]/2),\n",
    "                   1.,0] ;print(\"\\nRaw box:\",raw_box)\n",
    "        group_boxes.append(raw_box)\n",
    "    \n",
    "    # Cook boxes\n",
    "    cooked_boxes = (torch.tensor(group_boxes)\n",
    "                   if group_boxes\n",
    "                   else None)\n",
    "    \n",
    "    # Add group bounding boxes to YOLO call and output\n",
    "    #if group_boxes: r.update(torch.cat([r.boxes.data,cooked_boxes.cuda()]))\n",
    "    \n",
    "    # Call ultralytics Boxes function to make them into an ultralytics Boxes object\n",
    "    Group_Boxes = (Boxes(cooked_boxes.cpu().numpy(),r.orig_shape)\n",
    "                   if group_boxes\n",
    "                   else None)\n",
    "\n",
    "    # Update group tracker\n",
    "    #if group_boxes: grouptracker.update(Group_Boxes,r.orig_img)\n",
    "    if group_boxes:\n",
    "        # Velocity extraction from BoTSORT Kalman filter\n",
    "        _ = grouptracker.update(Group_Boxes,r.orig_img)\n",
    "        # Extract Kalman states\n",
    "        group_states = {\n",
    "            track.track_id: track.mean.copy()\n",
    "            for track in grouptracker.tracked_stracks\n",
    "            if track.is_activated\n",
    "        } ;print(group_states)\n",
    "        # TODO\n",
    "\n",
    "    # Save annotated frames\n",
    "    annotated_frame = annotator.result()\n",
    "    annotator.save(f\"runs/boxes{run}/annotated_frame_{frame}.png\")\n",
    "\n",
    "    # Save the image\n",
    "    if save_img==True:\n",
    "        # Saving the images of centroids and group ownership\n",
    "        plt.imshow(r.orig_img)\n",
    "        scatter = plt.scatter(x,y,c=uvxyz_labs,label=uvxyz_labs)\n",
    "        plt.quiver(x,y,u,v,angles = \"xy\")\n",
    "        plt.legend(*scatter.legend_elements(),title=\"Classes\")\n",
    "        plt.tight_layout\n",
    "        plt.savefig(f\"runs/dots{run}/frame_{frame}\",dpi=100)\n",
    "        plt.clf()\n",
    "\n",
    "    # Prepare groups for tracking group momentum\n",
    "    #pids = r.boxes.id.tolist() ;print(\"Person IDs:\",pids)\n",
    "    #pids = [x for x in kalman_states.keys()] ;print(\"Person IDs:\",pids)\n",
    "    #gmembs = {k: v for k, v in zip(pids, labs)} ;print(\"Group membership dictionary:\",gmembs)\n",
    "    #coords = {k: v for k, v in zip(pids, groups.T[:,:3].tolist())} ;print(\"XYZ coordinate dictionary:\",coords)\n",
    "    #velos = {k: v for k, v in zip(pids, lumps.T[:,3:].tolist())} ;print(\"XY velocity dictionary:\",velos)\n",
    "    #out_dicts = [pids,gmembs,coords,velos]\n",
    "    \n",
    "    # Testing out dictionaries\n",
    "    clumps = torch.vstack((xr,yr,d,u,v,torch.tensor(uvxyz_labs))) ;print(\"The clumps:\",clumps)\n",
    "    out_dicts = [{k:v for k,v in zip([\"x\",\"y\",\"z\",\"vx\",\"vy\",\"cluster\"],i)} for i in clumps.T.tolist()] ;print(out_dicts)\n",
    "\n",
    "    return out_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel grouping & depth & distance & velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLO11 detector\n",
    "model = YOLO(\"yolo11x.pt\")\n",
    "\n",
    "# Create tracker\n",
    "args = SimpleNamespace(\n",
    "    track_buffer=30,\n",
    "    track_high_thresh=0.25,\n",
    "    track_low_thresh=0.1,\n",
    "    match_thresh=0.8,\n",
    "    new_track_thresh=0.25,\n",
    "    fuse_score=True,\n",
    "    # BoT-SORT settings\n",
    "    gmc_method=\"sparseOptFlow\", # method of global motion compensation\n",
    "    # ReID model related thresh (not supported yet)\n",
    "    proximity_thresh=0.5,\n",
    "    appearance_thresh=0.25,\n",
    "    with_reid=False\n",
    ")\n",
    "tracker = BOTSORT(args, frame_rate=30)\n",
    "grouptracker = BOTSORT(args, frame_rate=30)\n",
    "\n",
    "\n",
    "# Begin online tracking\n",
    "results = model.predict(source=\"Homebrew-video/Low-quality/IMG_5354.MP4\",\n",
    "                      stream=True,classes=[0],half=True,imgsz=1280,save=False)\n",
    "frame=1\n",
    "\n",
    "#group_class = ObjectTracker()\n",
    "\n",
    "for j,r in enumerate(results):\n",
    "    # Move results onto gpu\n",
    "    r = r.cuda()\n",
    "\n",
    "    # testing\n",
    "    # print(r.boxes)\n",
    "    # custom_data = torch.tensor([\n",
    "    #     [ 50.0,  30.0, 200.0, 180.0, 0, 0.85, 0],\n",
    "    #     [120.0, 100.0, 300.0, 260.0, 1, 0.92, 0]\n",
    "    # ]) ;print(custom_data.shape)\n",
    "    # custom_boxes = Boxes(custom_data,r.orig_shape) ;print(custom_boxes)\n",
    "    # r.update(boxes=custom_data)\n",
    "    # print(r.boxes)\n",
    "    # if j==1: break\n",
    "    # continue\n",
    "\n",
    "    # Run depth inference\n",
    "    d,f = depth_pro_inference(r)\n",
    "\n",
    "    # Run grouping\n",
    "    out_dicts = theloop(r,d,f,tracker,grouptracker,save_img=False,run=8)\n",
    "    #group_class.update(out_dicts)\n",
    "    #print(group_class.get_tracked_objects())\n",
    "    \n",
    "\n",
    "    # Testing break\n",
    "    if j==5: break\n",
    "\n",
    "    frame += 1\n",
    "\n",
    "    print(\"\\nEND OF LOOP\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SUSTxSUBSEQUENT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
