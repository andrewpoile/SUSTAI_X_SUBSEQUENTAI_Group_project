{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import ultralytics\n",
    "ultralytics.checks()\n",
    "from ultralytics import YOLO\n",
    "model = YOLO(\"yolo11x.pt\")\n",
    "\n",
    "import torch\n",
    "import torch.version\n",
    "import torchvision\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"Torchvision version:\", torchvision.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "\n",
    "#from PIL import Image\n",
    "#import depth_pro\n",
    "\n",
    "# Load model and preprocessing transform\n",
    "#depth_model, transform = depth_pro.create_model_and_transforms(device=\"cuda\",precision=torch.float16)\n",
    "#depth_model.eval()\n",
    "\n",
    "#from sklearn.cluster import DBSCAN\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "from ultralytics import solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.track(source=\"videos/Pedestrians_1.mp4\",stream=True,classes=[0],save=True)\n",
    "for r in results:\n",
    "    print(r.orig_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speed and direction estimation\n",
    "cap = cv2.VideoCapture(\"videos/Video.mp4\")\n",
    "assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "# Video writer\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "video_writer = cv2.VideoWriter(\"Speed_Results/Video.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
    "\n",
    "# speed region points\n",
    "speed_region = [(1, 1), (w, 0), (w, h), (0, h)]\n",
    "\n",
    "# Initialize speed estimation object\n",
    "speedestimator = solutions.SpeedEstimator(\n",
    "    show=False,  # display the output\n",
    "    model=\"yolo11x.pt\",  # path to the YOLO11 model file.\n",
    "    region=speed_region,  # pass region points\n",
    "    classes=[0],  # estimate speed of specific classes.\n",
    "    #tracker = 'bytetrack.yaml',\n",
    ")\n",
    "\n",
    "# Process video\n",
    "while cap.isOpened():\n",
    "    success, im0 = cap.read()\n",
    "\n",
    "    if not success:\n",
    "        print(\"Video frame is empty or processing is complete.\")\n",
    "        break\n",
    "\n",
    "    results = speedestimator(im0)\n",
    "\n",
    "    video_writer.write(results.plot_im)  # write the processed frame.\n",
    "\n",
    "cap.release()\n",
    "video_writer.release()\n",
    "cv2.destroyAllWindows()  # destroy all opened windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instance segmentation and tracking\n",
    "cap = cv2.VideoCapture(\"videos/VID-20250322-WA0007.mp4\")\n",
    "assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "# Video writer\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "video_writer = cv2.VideoWriter(\"Speed_Results/VID-20250322-WA0007.mp4\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
    "\n",
    "# Initialize instance segmentation object\n",
    "isegment = solutions.InstanceSegmentation(\n",
    "    show=False,  # display the output\n",
    "    model=\"yolo11n-seg.pt\",  # model=\"yolo11n-seg.pt\" for object segmentation using YOLO11.\n",
    "    # classes=[0, 2],  # segment specific classes i.e, person and car with pretrained model.\n",
    ")\n",
    "\n",
    "# Process video\n",
    "while cap.isOpened():\n",
    "    success, im0 = cap.read()\n",
    "\n",
    "    if not success:\n",
    "        print(\"Video frame is empty or video processing has been successfully completed.\")\n",
    "        break\n",
    "\n",
    "    results = isegment(im0)\n",
    "\n",
    "    # print(results)  # access the output\n",
    "\n",
    "    video_writer.write(results.plot_im)  # write the processed frame.\n",
    "\n",
    "cap.release()\n",
    "video_writer.release()\n",
    "cv2.destroyAllWindows()  # destroy all opened windows\n",
    "\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "results = model.track(source=\"Speed_Results/VID-20250322-WA0007.mp4\",stream=True,classes=[0],save=True)\n",
    "for r in results:\n",
    "    print(r.orig_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.track(source=\"videos/VID-20250322-WA0007.mp4\",stream=True,classes=[0])\n",
    "for r in results:\n",
    "    #print(r.cuda())\n",
    "    # Load and preprocess an image.\n",
    "    #d_image, _, f_px = depth_pro.load_rgb(\"Homebrew-image/WIN_20250308_20_09_29_Pro.jpg\")\n",
    "    d_img = transform(r.orig_img).to(\"cuda\")\n",
    "    # Run inference.\n",
    "    prediction = depth_model.infer(d_img)\n",
    "    depth = prediction[\"depth\"]  # Depth in [m].\n",
    "    #focallength_px = prediction[\"focallength_px\"]  # Focal length in pixels.\n",
    "\n",
    "    print(depth.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess an image.\n",
    "image, _, f_px = depth_pro.load_rgb(\"Homebrew-image/WIN_20250308_20_09_29_Pro.jpg\")\n",
    "image = transform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference.\n",
    "prediction = depth_model.infer(image, f_px=f_px)\n",
    "depth = prediction[\"depth\"]  # Depth in [m].\n",
    "focallength_px = prediction[\"focallength_px\"]  # Focal length in pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(depth)\n",
    "depth = (depth - depth.min()) / depth.max()\n",
    "print(depth)\n",
    "# Remove the extra channel dimension if present.\n",
    "img_array = depth.cpu().numpy()\n",
    "\n",
    "# Convert the image from [0, 1] to [0, 255] and cast to uint8.\n",
    "img_array = (img_array*255).astype(np.uint8)\n",
    "\n",
    "# Create a PIL image in grayscale mode ('L').\n",
    "pil_img = Image.fromarray(img_array, mode='L')\n",
    "\n",
    "# Display the image using PIL's built-in viewer.\n",
    "pil_img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_l2_norms(centroids, threshold=100.0, device='cuda'):\n",
    "    \"\"\"\n",
    "    Vectorized function to calculate pairwise L2 norms between centroids (3D) on GPU\n",
    "    and return group memberships based on a distance threshold.\n",
    "\n",
    "    Parameters:\n",
    "    centroids (torch.Tensor): A tensor of shape (N, 3) representing object centroids (x, y, z) for the current frame.\n",
    "    threshold (float): The distance threshold to consider objects in the same group.\n",
    "    device (str): Device where the tensors are stored ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "    group_memberships (list of sets): List of sets, each set contains indices of objects that are in the same group.\n",
    "    \"\"\"\n",
    "    centroids = centroids.to(device)  # Move centroids tensor to the appropriate device\n",
    "\n",
    "    # Compute pairwise L2 distances using broadcasting in PyTorch\n",
    "    diff = centroids[:, None, :] - centroids[None, :, :]  # Shape (N, N, 3)\n",
    "    dist_matrix = torch.norm(diff, dim=2)  # Shape (N, N), compute the L2 norm along axis 2\n",
    "\n",
    "    # Create a list of groups based on the distance threshold\n",
    "    num_objects = centroids.shape[0]\n",
    "    group_memberships = []\n",
    "\n",
    "    for i in range(num_objects):\n",
    "        group = set([i])  # Start with the current object in its own group\n",
    "        for j in range(num_objects):\n",
    "            if i != j and dist_matrix[i, j] < threshold:\n",
    "                group.add(j)\n",
    "        group_memberships.append(group)\n",
    "\n",
    "    return group_memberships\n",
    "\n",
    "def track_groups(previous_groups, current_groups):\n",
    "    \"\"\"\n",
    "    Function to track if objects have joined or left groups between frames.\n",
    "\n",
    "    Parameters:\n",
    "    previous_groups (list of sets): List of sets from the previous frame's group memberships.\n",
    "    current_groups (list of sets): List of sets from the current frame's group memberships.\n",
    "\n",
    "    Returns:\n",
    "    joined (list): List of object indices that joined a new group.\n",
    "    left (list): List of object indices that left a group.\n",
    "    \"\"\"\n",
    "    joined = []\n",
    "    left = []\n",
    "\n",
    "    # Create sets to track which objects have changed groups\n",
    "    prev_indices = {frozenset(group) for group in previous_groups}\n",
    "    curr_indices = {frozenset(group) for group in current_groups}\n",
    "\n",
    "    # Detect objects joining new groups\n",
    "    for i, current_group in enumerate(current_groups):\n",
    "        if frozenset(current_group) not in prev_indices:\n",
    "            joined.append(i)\n",
    "\n",
    "    # Detect objects leaving groups\n",
    "    for i, prev_group in enumerate(previous_groups):\n",
    "        if frozenset(prev_group) not in curr_indices:\n",
    "            left.append(i)\n",
    "\n",
    "    return joined, left\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.track(source=\"Homebrew-video/Low-quality/IMG_5347.MP4\",stream=True,imgsz=1280,classes=[0],vid_stride=10)\n",
    "for r in results:\n",
    "    #print(r.cuda())\n",
    "    # Load and preprocess an image.\n",
    "    #d_image, _, f_px = depth_pro.load_rgb(\"Homebrew-image/WIN_20250308_20_09_29_Pro.jpg\")\n",
    "    d_img = transform(r.orig_img).to(\"cuda\")\n",
    "    # Run inference.\n",
    "    prediction = depth_model.infer(d_img)\n",
    "    depth = prediction[\"depth\"]  # Depth in [m].\n",
    "    #focallength_px = prediction[\"focallength_px\"]  # Focal length in pixels.\n",
    "\n",
    "    print(depth.shape)\n",
    "\n",
    "    richard = torch.empty(0,3).to(\"cuda\")\n",
    "    for i in r.boxes:\n",
    "        xy = i.xyxy[0]#; print(xy); print(xy[:2],xy[2:])\n",
    "        c = torch.tensor([ (xy[:2][0]+xy[2:][0])/2, (xy[2:][-1]+xy[:2][-1])/2 ], device=\"cuda\"); print(\"centroids\",c)\n",
    "        wh = i.xywh[0]# ;print(wh)\n",
    "        d = depth[int(c[1].item())][int(c[0].item())]; print(\"depth value at centroid (m)\",d)\n",
    "        #d = ((2*torch.pi*180)/(wh[2]+wh[3]*360)*1000+3); print(d)\n",
    "        #id = i.id; print(i.id)\n",
    "        \n",
    "        blobs = torch.hstack((c,d)); print(blobs)\n",
    "        richard = torch.vstack((richard,blobs))#; print(\"this is richard say hello:\", richard)\n",
    "    #dist = calculate_l2_norms(richard); print(\"these are the groups:\", dist)\n",
    "\n",
    "    clustering = DBSCAN(eps=100, min_samples=2).fit(richard.cpu().numpy())\n",
    "    print(clustering.labels_)\n",
    "\n",
    "    plt.scatter(richard.cpu().numpy()[:,0],richard.cpu().numpy()[:,1],c=clustering.labels_)\n",
    "    plt.xlim(0,1280)\n",
    "    plt.ylim(0,720)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.track(source=\"Homebrew-video/Low-quality/IMG_5347.MP4\",stream=True,classes=[0],half=False,imgsz=1280,vid_stride=1)\n",
    "frame=1\n",
    "for r in results:\n",
    "    #print(r.cuda())\n",
    "    # Load and preprocess an image.\n",
    "    #d_image, _, f_px = depth_pro.load_rgb(\"Homebrew-image/WIN_20250308_20_09_29_Pro.jpg\")\n",
    "    d_img = transform(r.orig_img).to(\"cuda\")\n",
    "    # Run inference.\n",
    "    prediction = depth_model.infer(d_img)\n",
    "    depth = prediction[\"depth\"]  # Depth in [m].\n",
    "    #focallength_px = prediction[\"focallength_px\"]  # Focal length in pixels.\n",
    "\n",
    "    print(depth.shape)\n",
    "\n",
    "    richard = torch.empty(0,3).to(\"cuda\")\n",
    "    for i in r.boxes:\n",
    "        xy = i.xyxy[0]#; print(xy); print(xy[:2],xy[2:])\n",
    "        c = torch.tensor([ (xy[:2][0]+xy[2:][0])/2, (xy[2:][-1]+xy[:2][-1])/2 ], device=\"cuda\"); print(\"centroids\",c)\n",
    "        wh = i.xywh[0]# ;print(wh)\n",
    "        d = depth[int(c[1].item())][int(c[0].item())]; print(\"depth value at centroid (m)\",d)\n",
    "        #d = ((2*torch.pi*180)/(wh[2]+wh[3]*360)*1000+3); print(d)\n",
    "        #id = i.id; print(i.id)\n",
    "        \n",
    "        blobs = torch.hstack((c,d)); print(blobs)\n",
    "        richard = torch.vstack((richard,blobs))#; print(\"this is richard say hello:\", richard)\n",
    "    #dist = calculate_l2_norms(richard); print(\"these are the groups:\", dist)\n",
    "\n",
    "    clustering = DBSCAN(eps=100, min_samples=2).fit(richard.cpu().numpy())\n",
    "    labs = clustering.labels_\n",
    "    print(labs)\n",
    "\n",
    "    \n",
    "    plt.imshow(r.orig_img)\n",
    "    scatter = plt.scatter(richard.cpu().numpy()[:,0],richard.cpu().numpy()[:,1],c=labs,label=labs)\n",
    "    plt.legend(*scatter.legend_elements(),\n",
    "               title=\"Classes\")\n",
    "    #plt.xlim(0,1280)\n",
    "    #plt.ylim(0,720)\n",
    "    plt.tight_layout\n",
    "    plt.savefig(f\"runs/dots/frame_{frame}\",dpi=200)\n",
    "    frame += 1\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.track(source=\"Homebrew-video/Low-quality/IMG_5347.MP4\",save=True,classes=[0],half=False,imgsz=1280,vid_stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.track(source=\"Homebrew-video/Low-quality/IMG_5347.MP4\",save=True,classes=[0],half=False,imgsz=1280,vid_stride=1,iou=0.9,conf=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subsequent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
